{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WorqHat/Langchain-Example/blob/main/Langchain-AiCon-V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For a comprehensive understanding of the `AiCon V2` and its integration with Langchain, we highly recommend referring to the official documentation and API reference. The documentation provides detailed insights into the model's architecture, features, and usage guidelines, ensuring you can leverage its full potential in your projects. Additionally, the API reference offers a deep dive into the available endpoints, request parameters, and response formats, facilitating a smooth integration process. Explore the [AiCON V2 documentation](https://docs.worqhat.com/ai-models/text-generation-ai/aicon-v2-textgen) for model-specific details and the [AiCON V2 documentation](https://docs.worqhat.com/api-reference/ai-models/text-generation-ai/aicon-v2-textgen) for API specifics to get started.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jJjC5UQhwLY",
        "outputId": "bc50eea7-69cb-49ce-dcbe-2eff7c421104"
      },
      "outputs": [],
      "source": [
        "%pip install langchain_core\n",
        "%pip install json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWYrwFLlf5JP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from typing import Any, List, Mapping, Optional\n",
        "from langchain_core.language_models.llms import LLM\n",
        "from langchain_core.callbacks.manager import CallbackManagerForLLMRun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPxt4793iOwt"
      },
      "outputs": [],
      "source": [
        "class CustomLLM(LLM):\n",
        "    \"\"\"\n",
        "    CustomLLM is a custom language model that interfaces with the Worqhat API to generate text based on a prompt.\n",
        "    It supports additional parameters for fine-tuning the response, including training data for few-shot learning,\n",
        "    control over the randomness of the response, and the ability to preserve conversation history.\n",
        "\n",
        "    Attributes:\n",
        "        n (int): An identifying parameter for instances of this class.\n",
        "    \"\"\"\n",
        "\n",
        "    n: int\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Defines the type of the language model.\"\"\"\n",
        "        return \"custom\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Generates a response to the given prompt using the Worqhat API.\n",
        "\n",
        "        Parameters:\n",
        "            prompt (str): The input prompt for the language model.\n",
        "            stop (Optional[List[str]]): A list of stop sequences. Not permitted in this implementation.\n",
        "            run_manager (Optional[CallbackManagerForLLMRun]): A manager for callbacks during the run. Not used.\n",
        "            **kwargs (Any): Additional parameters supported by the API, such as training_data, randomness, etc.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated response from the API.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If stop sequences are provided.\n",
        "            HTTPError: If the API request fails.\n",
        "        \"\"\"\n",
        "        if stop is not None:\n",
        "            raise ValueError(\"stop kwargs are not permitted.\")\n",
        "\n",
        "        url = \"https://api.worqhat.com/api/ai/content/v2\"\n",
        "\n",
        "        # Indicate that you expect a streamable response if 'stream_data' is True\n",
        "        stream_data = kwargs.get(\"stream_data\", True)\n",
        "\n",
        "        payload = json.dumps({\n",
        "            \"question\": prompt,\n",
        "            \"training_data\": kwargs.get(\"training_data\", \"your name is johnny. when user asks whats your name. say johnny\"),\n",
        "            \"preserve_history\": kwargs.get(\"preserve_history\", False),\n",
        "            \"stream_data\": stream_data,\n",
        "            \"history_object\": kwargs.get(\"history_object\", []),\n",
        "            \"randomness\": kwargs.get(\"randomness\", 0.5),\n",
        "            \"response_type\": kwargs.get(\"response_type\", \"text\"),  # The type of response to generate. Options: \"text\", \"json\"\n",
        "            **kwargs\n",
        "        })\n",
        "\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json',\n",
        "            'Accept': 'application/json',\n",
        "            'Authorization': f'Bearer sk-02e44d2ccb164c738a6c4a65dbf75e89'  # Load API key from environment variable\n",
        "        }\n",
        "\n",
        "        response = requests.post(url, headers=headers, data=payload, stream=stream_data)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            if stream_data:\n",
        "                # Process the streamable response\n",
        "                for line in response.iter_lines():\n",
        "                    if line:\n",
        "                        decoded_line = line.decode('utf-8')\n",
        "                        print(decoded_line)  # Or handle the streaming data as needed\n",
        "                return \"Stream processed successfully.\"\n",
        "            else:\n",
        "                return response.text\n",
        "        else:\n",
        "            response.raise_for_status()\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        \"\"\"\n",
        "        Returns identifying parameters for instances of this class.\n",
        "\n",
        "        Returns:\n",
        "            Mapping[str, Any]: A dictionary of identifying parameters.\n",
        "        \"\"\"\n",
        "        return {\"n\": self.n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZZyBh2wiOqj"
      },
      "outputs": [],
      "source": [
        "llm = CustomLLM(n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "xcI4C90dzsGd",
        "outputId": "2278d9df-a596-4968-d6d2-85b3e7a03ef6"
      },
      "outputs": [],
      "source": [
        "answer = llm.invoke(\"who created you?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNgYRCbl6TshLEE7KYekwJa",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
